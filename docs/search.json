[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Squirrels\n\n\n1 min\n\n\n\nPyMC\n\n\n\nSquirrels in Central Park\n\n\n\nMads Chr. Hansen\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes Course\n\n\n1 min\n\n\n\nBayes\n\n\n\nPart 1\n\n\n\nMads Chr. Hansen\n\n\nOct 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFlight Delays\n\n\n1 min\n\n\n\nPyMC\n\n\n\nAnalyzing flight delays with PyMC\n\n\n\nMads Chr. Hansen\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\n\n\n1 min\n\n\n\nPyMC\n\n\n\nAnalyzing height with PyMC\n\n\n\nMads Chr. Hansen\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Rethinking\n\n\n1 min\n\n\n\nPyMC\n\n\nBooks\n\n\n\nBayesian Course by Richard McElreath\n\n\n\nMads Chr. Hansen\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayes14",
    "section": "",
    "text": "\\[p(\\theta|y)=\\frac{p(y|\\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta)\\,d\\theta}\\]\n\n\nInvestigating Bayesian Statistics in Python"
  },
  {
    "objectID": "posts/squirrels/eda.html",
    "href": "posts/squirrels/eda.html",
    "title": "Squirrels",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport folium\nfrom folium import plugins\nimport geopandas \nimport matplotlib\nimport pymc as pm\n\n\n\n\nCode\nsquirrel_data = geopandas.read_file('../data/central_park.geojson')\n\n\n\n\nCode\nsquirrel_data.head()\n\n\n\n\n\n\n\n\n\nx\nmoans\nprimary_fur_color\nhectare\nforaging\nspecific_location\nlocation\nunique_squirrel_id\ny\nrunning\n...\ndate\nhectare_squirrel_number\nquaas\napproaches\neating\ncombination_of_primary_and\nchasing\nother_interactions\ntail_flags\ngeometry\n\n\n\n\n0\n-73.9561344937861\nfalse\nNone\n37F\nfalse\nNone\nNone\n37F-PM-1014-03\n40.7940823884086\nfalse\n...\n10142018\n3\nfalse\nfalse\nfalse\n+\nfalse\nNone\nfalse\nPOINT (-73.95613 40.79408)\n\n\n1\n-73.9688574691102\nfalse\nNone\n21B\nfalse\nNone\nNone\n21B-AM-1019-04\n40.7837825208444\nfalse\n...\n10192018\n4\nfalse\nfalse\nfalse\n+\nfalse\nNone\nfalse\nPOINT (-73.96886 40.78378)\n\n\n2\n-73.9742811484852\nfalse\nGray\n11B\nfalse\nNone\nAbove Ground\n11B-PM-1014-08\n40.775533619083\nfalse\n...\n10142018\n8\nfalse\nfalse\nfalse\nGray+\ntrue\nNone\nfalse\nPOINT (-73.97428 40.77553)\n\n\n3\n-73.9596413903948\nfalse\nGray\n32E\ntrue\nNone\nNone\n32E-PM-1017-14\n40.7903128889029\nfalse\n...\n10172018\n14\nfalse\nfalse\ntrue\nGray+\nfalse\nNone\nfalse\nPOINT (-73.95964 40.79031)\n\n\n4\n-73.9702676472613\nfalse\nGray\n13E\ntrue\non tree stump\nAbove Ground\n13E-AM-1017-05\n40.7762126854894\nfalse\n...\n10172018\n5\nfalse\nfalse\nfalse\nGray+Cinnamon\nfalse\nNone\nfalse\nPOINT (-73.97027 40.77621)\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\nCode\nmap = folium.Map(location=[40.779935868085204, -73.96693897913316], tiles=\"CartoDB Positron\", zoom_start=13)\nheat_data = [[point.xy[1][0], point.xy[0][0]] for point in squirrel_data.geometry]\nplugins.HeatMap(heat_data, radius=12, blur=7).add_to(map)\nmap\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\nCode\nsquirrel_data.unique_squirrel_id.unique().shape\n\n\n(3018,)"
  },
  {
    "objectID": "posts/height/height.html",
    "href": "posts/height/height.html",
    "title": "Height",
    "section": "",
    "text": "Analysis of Height\nWe will analyze height data (Lindley 1983)\n\nTheorem 1 (Line) The equation of any straight line, called a linear equation, can be written as:\n\\[\ny = mx + b\n\\]\n\nSee Theorem 1.\n\nProof. Here is a proof\n\n\n\n\n\n\n\nTip with Title\n\n\n\nThis is an example of a callout with a title.\n\n\n\n\nCode\nimport pandas as pd\nimport arviz as az\nimport graphviz as gr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\nimport xarray as xr\nfrom scipy import stats\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\naz.style.use([\"arviz-darkgrid\", \"arviz-viridish\"])\nplt.rcParams[\"figure.figsize\"] = [10, 6]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"figure.facecolor\"] = \"white\"\n\n%reload_ext watermark\n%reload_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = \"retina\"\n\n\n\n\nCode\ndata = pd.read_csv('https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv', sep=';')\ndata.head()\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n0\n151.765\n47.825606\n63.0\n1\n\n\n1\n139.700\n36.485807\n63.0\n0\n\n\n2\n136.525\n31.864838\n65.0\n0\n\n\n3\n156.845\n53.041914\n41.0\n1\n\n\n4\n145.415\n41.276872\n51.0\n0\n\n\n\n\n\n\n\n\n\nCode\nsns.pairplot(data, hue='male')\n\n\n\n\n\n\n\nCode\nsns.scatterplot(data, x='height', y='weight', alpha=0.7, hue='male');\n\n\n\n\n\n\n\nCode\nsns.scatterplot(data, x='age', y='weight', alpha=0.7, hue='male');\n\n\n\n\n\nIt seems that there is a linear relationship from age 0 to 18 and then it levels out. Males at a higher level.\n\n\nCode\ng = gr.Digraph()\ng.node(name=\"height\", label=\"height\", style=\"filled\")\ng.node(name=\"weight\", label=\"weight\", style=\"filled\")\ng.node(name=\"male\", label=\"male\", style=\"filled\")\ng.node(name=\"age\", label=\"age\", style=\"filled\")\ng.edge(tail_name=\"height\", head_name=\"weight\")\ng.edge(tail_name=\"age\", head_name=\"height\")\ng.edge(tail_name=\"male\", head_name=\"height\")\ng.edge(tail_name=\"male\", head_name=\"weight\")\ng\n\n\n\n\n\n\n\nBasic Model\nNo regressors\n\n\nCode\nsns.histplot(data, x='weight', kde=True, hue='male');\nplt.title('Histogram of Weight')\n\n\nText(0.5, 1.0, 'Histogram of Weight')\n\n\n\n\n\n\n\nCode\nwith pm.Model() as basic_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=30, sigma=1)\n    obs = pm.Normal(\"mass\", mu=mu, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_basic = pm.sample()\n    pm.compute_log_likelihood(idata_basic, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_basic, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_basic, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nmale_idx, male = data[\"male\"].astype('category').factorize(sort=True)\n\n\n\n\nCode\ncoords = {\n    \"male\": male,\n    \"obs\": range(len(data))\n}\n\n\n\n\nCode\nwith pm.Model(coords=coords) as male_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=30, sigma=1, dims=\"male\")\n    obs = pm.Normal(\"mass\", mu=mu[male_idx], sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_male = pm.sample()\n    pm.compute_log_likelihood(idata_male, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_male, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_male, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as age_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.age, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_age = pm.sample()\n    pm.compute_log_likelihood(idata_age, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_age, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_age, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.height, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_height = pm.sample()\n    pm.compute_log_likelihood(idata_height, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_height, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_exp_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    e = pm.Normal('e', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.height.values ** e, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_height_exp = pm.sample()\n    pm.compute_log_likelihood(idata_height_exp, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height_exp, extend_inferencedata=True)\n\n\nSampling: [e, mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu, e]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 24 seconds.\nThere were 2 divergences after tuning. Increase `target_accept` or reparameterize.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:23&lt;00:00 Sampling 4 chains, 2 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(idata_height_exp)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1048.0\n1159.0\n1.0\n\n\ne\n2.613\n0.045\n2.531\n2.699\n0.001\n0.001\n1048.0\n1156.0\n1.0\n\n\nsigma\n3.864\n0.108\n3.670\n4.072\n0.003\n0.002\n1785.0\n1734.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.plot_ppc(data=idata_height_exp, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_exp_age_model:\n    height = pm.ConstantData('height', data.height.values)\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2, dims='male')\n    e = pm.Normal('e', mu=1, sigma=2, dims='male')\n    obs = pm.TruncatedNormal(\"mass\", mu=mu[male_idx]*height ** e[male_idx], sigma=sigma, lower=0, observed=data.weight, dims='obs')\n    \n    prior = pm.sample_prior_predictive()\n    idata_height_exp_male = pm.sample()\n    pm.compute_log_likelihood(idata_height_exp_male, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height_exp_male, extend_inferencedata=True)\n\n\nSampling: [e, mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu, e]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 145 seconds.\nThere were 4 divergences after tuning. Increase `target_accept` or reparameterize.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 02:25&lt;00:00 Sampling 4 chains, 4 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(idata_height_exp_male)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu[0]\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1441.0\n1453.0\n1.0\n\n\nmu[1]\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1352.0\n1300.0\n1.0\n\n\ne[0]\n2.768\n0.081\n2.619\n2.922\n0.002\n0.002\n1442.0\n1426.0\n1.0\n\n\ne[1]\n2.623\n0.069\n2.488\n2.749\n0.002\n0.001\n1351.0\n1304.0\n1.0\n\n\nsigma\n3.867\n0.110\n3.675\n4.086\n0.003\n0.002\n1899.0\n1095.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.plot_ppc(data=idata_height_exp_male, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\naz.plot_lm(idata=idata_height_exp_male, y='mass', x='height')\n\n\narray([[&lt;Axes: xlabel='height', ylabel='mass'&gt;]], dtype=object)\n\n\n\n\n\n\n\nCode\n%watermark\n\n\nLast updated: 2023-10-09T23:52:57.249863+02:00\n\nPython implementation: CPython\nPython version       : 3.11.5\nIPython version      : 8.15.0\n\nCompiler    : Clang 14.0.3 (clang-1403.0.22.14.1)\nOS          : Darwin\nRelease     : 22.6.0\nMachine     : arm64\nProcessor   : arm\nCPU cores   : 10\nArchitecture: 64bit\n\n\n\n\n\n\n\n\nReferences\n\nLindley, Dennis V. 1983. “Theory and Practice of Bayesian Statistics.” The Statistician 32 (March): 1. https://doi.org/10.2307/2987587."
  },
  {
    "objectID": "posts/flights/flight_delays.html",
    "href": "posts/flights/flight_delays.html",
    "title": "Flight Delays",
    "section": "",
    "text": "Code\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\n\n\n\n\nCode\ndf = pd.read_csv(\"948363589_T_ONTIME_MARKETING.zip\")\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\nYEAR\nQUARTER\nMONTH\nDAY_OF_MONTH\nDAY_OF_WEEK\nFL_DATE\nMKT_UNIQUE_CARRIER\nBRANDED_CODE_SHARE\nMKT_CARRIER_AIRLINE_ID\nMKT_CARRIER\n...\nARR_DELAY\nARR_DELAY_NEW\nARR_DEL15\nARR_DELAY_GROUP\nARR_TIME_BLK\nCANCELLED\nCANCELLATION_CODE\nDIVERTED\nDUP\nUnnamed: 60\n\n\n\n\n0\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n6.0\n6.0\n0.0\n0.0\n1100-1159\n0.0\nNaN\n0.0\nN\nNaN\n\n\n1\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-21.0\n0.0\n0.0\n-2.0\n2100-2159\n0.0\nNaN\n0.0\nN\nNaN\n\n\n2\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n10.0\n10.0\n0.0\n0.0\n1900-1959\n0.0\nNaN\n0.0\nN\nNaN\n\n\n3\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-10.0\n0.0\n0.0\n-1.0\n0900-0959\n0.0\nNaN\n0.0\nN\nNaN\n\n\n4\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-10.0\n0.0\n0.0\n-1.0\n1300-1359\n0.0\nNaN\n0.0\nN\nNaN\n\n\n\n\n5 rows × 61 columns\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(10,4))\n\nmsn_arrivals = df[(df[\"DEST\"] == \"MSN\") & df[\"ORIGIN\"].isin([\"MSP\", \"DTW\"])][\"ARR_DELAY\"]\n\naz.plot_kde(msn_arrivals.values, ax=ax, bw=10)\nax.set_yticks([])\nax.set_xlabel(\"Minutes late\")\n\n\nText(0.5, 0, 'Minutes late')\n\n\n\n\n\n\n\nCode\nwith pm.Model() as normal_model:\n    normal_sd = pm.HalfStudentT(\"sd\",sigma=60, nu=5)\n    normal_mu = pm.Normal(\"mu\", 0, 30) \n\n    normal_delay = pm.Normal(\"delays\",mu=normal_mu,\n                             sigma=normal_sd, observed=msn_arrivals)\n    normal_prior_predictive = pm.sample_prior_predictive()\n    \nwith pm.Model() as gumbel_model:\n    gumbel_beta = pm.HalfStudentT(\"beta\", sigma=60, nu=5)\n    gumbel_mu = pm.Normal(\"mu\", 0, 40)\n    \n    gumbel_delays = pm.Gumbel(\"delays\",\n                              mu=gumbel_mu,\n                              beta=gumbel_beta,\n                              observed=msn_arrivals)\n    gumbel_prior_predictive = pm.sample_prior_predictive()\n\n\nSampling: [delays, mu, sd]\nSampling: [beta, delays, mu]\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nprior_predictives = {\"normal\":normal_prior_predictive.prior_predictive, \"gumbel\": gumbel_prior_predictive.prior_predictive}\n\nfor i, (label, prior_predictive) in enumerate(prior_predictives.items()):\n    \n    data = prior_predictive[\"delays\"]\n    az.plot_dist(data, ax=axes[i])\n    axes[i].set_yticks([])\n    axes[i].set_xlim(-300, 300)\n    axes[i].set_title(label)\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    normal_delay_trace = pm.sample(random_seed=0, chains=2)\naz.plot_rank(normal_delay_trace)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [sd, mu]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\narray([&lt;Axes: title={'center': 'mu'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;,\n       &lt;Axes: title={'center': 'sd'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\nwith gumbel_model:\n    gumbel_delay_trace = pm.sample(chains=2)\naz.plot_rank(gumbel_delay_trace)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [beta, mu]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 0 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\narray([&lt;Axes: title={'center': 'mu'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;,\n       &lt;Axes: title={'center': 'beta'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\naz.plot_posterior(normal_delay_trace)\n\n\narray([&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'sd'}&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\naz.plot_posterior(gumbel_delay_trace)\n\n\narray([&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'beta'}&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    normal_delay_trace = pm.sample(random_seed=0)\n    pm.sample_posterior_predictive(normal_delay_trace, extend_inferencedata=True)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sd, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [delays]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(normal_delay_trace, num_pp_samples=100)\n\n\n&lt;Axes: xlabel='delays / delays'&gt;\n\n\n\n\n\n\n\nCode\nwith gumbel_model:\n    gumbel_delay_trace = pm.sample(random_seed=0)\n    pm.sample_posterior_predictive(gumbel_delay_trace, extend_inferencedata=True)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 0 seconds.\nSampling: [delays]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(gumbel_delay_trace, num_pp_samples=100)\n\n\n&lt;Axes: xlabel='delays / delays'&gt;\n\n\n\n\n\n\n\nCode\ngumbel_late = gumbel_delay_trace.posterior_predictive[\"delays\"].values.reshape(-1, 336).copy()\ndist_of_late = (gumbel_late &gt; 0).sum(axis=1) / 336\n\npercent_observed_late = (msn_arrivals &gt; 0).sum() / 336\n\n\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize=(12,4))\naz.plot_dist(dist_of_late, ax=axes[0])\n\n\naxes[0].axvline(percent_observed_late, c=\"gray\")\naxes[0].set_title(\"Test Statistic of On Time Proportion\")\naxes[0].set_yticks([])\n\ngumbel_late[gumbel_late &lt; 0] = np.nan\nmedian_lateness = np.nanmedian(gumbel_late, axis=1)\naz.plot_dist(median_lateness,  ax=axes[1])\n\nmedian_time_observed_late = msn_arrivals[msn_arrivals &gt;= 0].median()\naxes[1].axvline(median_time_observed_late, c=\"gray\")\naxes[1].set_title(\"Test Statistic of Median Minutes Late\")\naxes[1].set_yticks([])\n\n\n[]\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    pm.compute_log_likelihood(normal_delay_trace)\n    \nwith gumbel_model:\n    pm.compute_log_likelihood(gumbel_delay_trace)\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\ncompare = az.compare({\"normal\": normal_delay_trace, \"gumbel\": gumbel_delay_trace}, ic=\"loo\")\ncompare\n\n\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\ngumbel\n0\n-1410.330975\n5.866211\n0.000000\n1.000000e+00\n45.179994\n0.000000\nFalse\nlog\n\n\nnormal\n1\n-1653.563218\n21.263250\n243.232243\n1.609379e-09\n65.204290\n27.489639\nTrue\nlog\n\n\n\n\n\n\n\n\n\nCode\n_, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\nfor label, model, ax in zip((\"gumbel\", \"normal\"),(gumbel_delay_trace, normal_delay_trace), axes):\n    az.plot_loo_pit(model, y=\"delays\", legend=False, use_hdi=True, ax=ax)\n    ax.set_title(label)\n\n\n\n\n\n\n\nCode\naz.plot_compare(compare);\n\n\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/plots/backends/matplotlib/compareplot.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  scale = comp_df[\"scale\"][0]\n\n\n\n\n\n\n\nCode\n@np.vectorize\ndef current_revenue(delay):\n    if delay &gt;= 0:\n        return 300 * delay\n    return np.nan\n\n\n\n\nCode\nposterior_pred = gumbel_delay_trace.posterior_predictive[\"delays\"].values.reshape(-1, 336).copy()\n\n\n\n\nCode\ndef revenue_calculator(posterior_pred, revenue_func):    \n    revenue_per_flight = revenue_func(posterior_pred)\n    average_revenue = np.nanmean(revenue_per_flight)\n    return revenue_per_flight, average_revenue\n\nrevenue_per_flight, average_revenue = revenue_calculator(posterior_pred,\ncurrent_revenue)\naverage_revenue\n\n\n3929.7925096256663\n\n\n\n\nCode\nfig, ax = plt.subplots()\nax.hist(revenue_per_flight.flatten(), bins=30, rwidth=.9, color=\"C2\" )\nax.set_yticks([])\nax.set_title(\"Late fee revenue per flight under current fee structure\")\nax.xaxis.set_major_formatter('${x:1.0f}')\n\n\n\n\n\n\n\nCode\n@np.vectorize\ndef proposed_revenue(delay):\n    \"\"\"Calculate proposed revenue for each delay \"\"\"\n    if delay &gt;= 100:\n        return 30000\n    elif delay &gt;= 10:\n        return 5000\n    elif delay &gt;= 0:\n        return 1000\n    else:\n        return np.nan\nrevenue_per_flight_proposed, average_revenue_proposed = revenue_calculator(posterior_pred, proposed_revenue)\n\n\n\n\nCode\naverage_revenue_proposed\n\n\n2929.739994157172\n\n\n\n\nCode\nfig, ax = plt.subplots()\n\ncounts = pd.Series(revenue_per_flight_proposed.flatten()).value_counts()\ncounts.index = counts.index.astype(int)\n\ncounts.plot(kind=\"bar\", ax=ax, color=\"C2\")\nax.set_title(\"Late fee revenue per flight under proposed fee structure\")\nax.set_yticks([]);\nax.tick_params(axis='x', labelrotation = 0)\nax.set_xticklabels([f\"${i}\" for i in counts.index])\n\n\n[Text(0, 0, '$1000'), Text(1, 0, '$5000'), Text(2, 0, '$30000')]\n\n\n\n\n\n\n\nCode\n!quarto render flight_delays.ipynb --to html\n\n\npandoc \n  to: html\n  output-file: flight_delays.html\n  standalone: true\n  section-divs: true\n  html-math-method: mathjax\n  wrap: none\n  default-image-extension: png\n  \nmetadata\n  document-css: false\n  link-citations: true\n  date-format: long\n  lang: en\n  \nOutput created: flight_delays.html"
  },
  {
    "objectID": "posts/statistical_rethinking/statistical_rethinking.html",
    "href": "posts/statistical_rethinking/statistical_rethinking.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "Richard McElreath has done a remarkable job in explaining Bayesian statistics in his book “Statistical Rethinking”\n\n\nLectures\nHis lectures are highly recommended as well.\n\nYou can find PyMC resources here.\n\n\nJust testing\n\\[\\int_{1}^{5}x^2\\,dx\\]"
  },
  {
    "objectID": "posts/bayes_course/bayes_course.html",
    "href": "posts/bayes_course/bayes_course.html",
    "title": "Bayes Course",
    "section": "",
    "text": "This presentation follows Gelman et al. (2014) closely. Bayesian inference is about learning unobserved parameters \\(\\theta\\) or data \\(\\tilde{y}\\) though probability. These probability statements are conditional on the observed data \\(y\\), that is \\(p(\\theta|y)\\) or \\(p(\\tilde{y}|y)\\).\n\n\n\n\n\n\n\n\nh"
  },
  {
    "objectID": "posts/bayes_course/bayes_course.html#likelihood",
    "href": "posts/bayes_course/bayes_course.html#likelihood",
    "title": "Bayes Course",
    "section": "",
    "text": "h"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a mathematician (PhD from University of Copenhagen) and data scientist (Maersk). On this site you can find more information about me and some of my projects."
  },
  {
    "objectID": "about.html#research-and-interests",
    "href": "about.html#research-and-interests",
    "title": "About",
    "section": "Research and Interests",
    "text": "Research and Interests\nMy research interests lie within dynamical systems theory, both deterministic and stochastic, especially within reaction networks.\nI also enjoy working on statistical properties of dynamical systems within time-series forecasting, and I’m interested in all aspects of Bayesian data analysis.\n\nPapers:\n\nExistence of a unique quasi-stationary distribution in stochastic reaction networks, Electron. J. Probab. 25: 1-30 (2020)\nStructural classification of continuous time Markov chains with applications, Stochastics, 94:7, 1003-1030 (2022)\nFull classification of dynamics for one-dimensional continuous-time Markov chains with polynomial transition rates, Advances in Applied Probability , Volume 55, Issue 1, pp. 321 - 355 (2023)\nThe asymptotic tails of limit distributions of continuous time Markov chains, Advances in Applied Probability Volume 56, Issue 2 (2024)\n\n\n\nTech Stack:"
  }
]