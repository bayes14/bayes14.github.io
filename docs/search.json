[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Flight Delays\n\n\n\nPyMC\n\n\n\n\n\n\n\nMads Chr. Hansen\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\n\n\n\nPyMC\n\n\n\n\n\n\n\nMads Chr. Hansen\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\nTristan Oâ€™Malley\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "posts/height/height.html",
    "href": "posts/height/height.html",
    "title": "Height",
    "section": "",
    "text": "Analysis of Height\n\n\nCode\nimport pandas as pd\nimport arviz as az\nimport graphviz as gr\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\nimport xarray as xr\nfrom scipy import stats\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\naz.style.use([\"arviz-darkgrid\"])\nplt.rcParams[\"figure.figsize\"] = [10, 6]\nplt.rcParams[\"figure.dpi\"] = 100\nplt.rcParams[\"figure.facecolor\"] = \"white\"\n\n%load_ext autoreload\n%autoreload 2\n%config InlineBackend.figure_format = \"retina\"\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nCode\ndata = pd.read_csv('https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Howell1.csv', sep=';')\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\nmale\n\n\n\n\n0\n151.765\n47.825606\n63.0\n1\n\n\n1\n139.700\n36.485807\n63.0\n0\n\n\n2\n136.525\n31.864838\n65.0\n0\n\n\n3\n156.845\n53.041914\n41.0\n1\n\n\n4\n145.415\n41.276872\n51.0\n0\n\n\n\n\n\n\n\n\n\nCode\nsns.pairplot(data, hue='male')\n\n\n\n\n\n\n\nCode\nsns.scatterplot(data, x='height', y='weight', alpha=0.7, hue='male');\n\n\n\n\n\n\n\nCode\nsns.scatterplot(data, x='age', y='weight', alpha=0.7, hue='male');\n\n\n\n\n\nIt seems that there is a linear relationship from age 0 to 18 and then it levels out. Males at a higher level.\n\n\nCode\ng = gr.Digraph()\ng.node(name=\"height\", label=\"height\", color=\"deepskyblue\", style=\"filled\")\ng.node(name=\"weight\", label=\"weight\", color=\"deepskyblue\", style=\"filled\")\ng.node(name=\"male\", label=\"male\", color=\"deepskyblue\", style=\"filled\")\ng.node(name=\"age\", label=\"age\", color=\"deepskyblue\", style=\"filled\")\ng.edge(tail_name=\"height\", head_name=\"weight\")\ng.edge(tail_name=\"age\", head_name=\"height\")\ng.edge(tail_name=\"male\", head_name=\"height\")\ng.edge(tail_name=\"male\", head_name=\"weight\")\ng\n\n\n\n\n\n\n\nBasic Model\nNo regressors\n\n\nCode\nsns.histplot(data, x='weight', kde=True, hue='male');\nplt.title('Histogram of Weight')\n\n\nText(0.5, 1.0, 'Histogram of Weight')\n\n\n\n\n\n\n\nCode\nwith pm.Model() as basic_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=30, sigma=1)\n    obs = pm.Normal(\"mass\", mu=mu, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_basic = pm.sample()\n    pm.compute_log_likelihood(idata_basic, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_basic, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_basic, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nmale_idx, male = data[\"male\"].astype('category').factorize(sort=True)\n\n\n\n\nCode\ncoords = {\n    \"male\": male,\n    \"obs\": range(len(data))\n}\n\n\n\n\nCode\nwith pm.Model(coords=coords) as male_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=30, sigma=1, dims=\"male\")\n    obs = pm.Normal(\"mass\", mu=mu[male_idx], sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_male = pm.sample()\n    pm.compute_log_likelihood(idata_male, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_male, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_male, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as age_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.age, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_age = pm.sample()\n    pm.compute_log_likelihood(idata_age, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_age, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_age, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.height, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_height = pm.sample()\n    pm.compute_log_likelihood(idata_height, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height, extend_inferencedata=True)\n\n\nSampling: [mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(data=idata_height, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_exp_model:\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2)\n    e = pm.Normal('e', mu=1, sigma=2)\n    obs = pm.Normal(\"mass\", mu=mu*data.height.values ** e, sigma=sigma, observed=data.weight)\n    \n    prior = pm.sample_prior_predictive()\n    idata_height_exp = pm.sample()\n    pm.compute_log_likelihood(idata_height_exp, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height_exp, extend_inferencedata=True)\n\n\nSampling: [e, mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu, e]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 21 seconds.\nThere were 5 divergences after tuning. Increase `target_accept` or reparameterize.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:20&lt;00:00 Sampling 4 chains, 5 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(idata_height_exp)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n976.0\n1144.0\n1.0\n\n\ne\n2.609\n0.048\n2.524\n2.702\n0.002\n0.001\n976.0\n1118.0\n1.0\n\n\nsigma\n3.870\n0.108\n3.676\n4.084\n0.003\n0.002\n1652.0\n1581.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\naz.plot_ppc(data=idata_height_exp, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\naz.compare({'basic': idata_basic, 'age': idata_age, 'male': idata_male, 'height': idata_height, 'heigh_exp': idata_height_exp, 'height_exp_male': idata_height_exp_male})\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nheight_exp_male\n0\n-1524.374838\n5.303433\n0.000000\n8.665450e-01\n20.521787\n0.000000\nFalse\nlog\n\n\nheigh_exp\n1\n-1528.789340\n3.374519\n4.414502\n1.245465e-01\n20.625731\n3.742969\nFalse\nlog\n\n\nheight\n2\n-1925.369594\n1.437219\n400.994756\n8.908504e-03\n12.588644\n21.859826\nFalse\nlog\n\n\nmale\n3\n-2244.043232\n2.146733\n719.668394\n0.000000e+00\n12.727979\n23.946396\nFalse\nlog\n\n\nbasic\n4\n-2244.925171\n1.502977\n720.550334\n0.000000e+00\n13.597773\n24.712621\nFalse\nlog\n\n\nage\n5\n-2305.565617\n2.804780\n781.190779\n2.322644e-10\n18.062763\n24.002201\nFalse\nlog\n\n\n\n\n\n\n\n\n\nCode\nwith pm.Model(coords=coords) as height_exp_age_model:\n    height = pm.ConstantData('height', data.height.values)\n    sigma = pm.Exponential(\"sigma\", 10)\n    mu = pm.Normal('mu', mu=1, sigma=2, dims='male')\n    e = pm.Normal('e', mu=1, sigma=2, dims='male')\n    obs = pm.TruncatedNormal(\"mass\", mu=mu[male_idx]*height ** e[male_idx], sigma=sigma, lower=0, observed=data.weight, dims='obs')\n    \n    prior = pm.sample_prior_predictive()\n    idata_height_exp_male = pm.sample()\n    pm.compute_log_likelihood(idata_height_exp_male, extend_inferencedata=True)\n    pm.sample_posterior_predictive(idata_height_exp_male, extend_inferencedata=True)\n\n\nSampling: [e, mass, mu, sigma]\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sigma, mu, e]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 123 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nThere were 74 divergences after tuning. Increase `target_accept` or reparameterize.\nSampling: [mass]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 02:03&lt;00:00 Sampling 4 chains, 74 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.summary(idata_height_exp_male)\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nmu[0]\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n92.0\n24.0\n1.03\n\n\nmu[1]\n0.000\n0.000\n0.000\n0.000\n0.000\n0.000\n1140.0\n1345.0\n1.01\n\n\ne[0]\n2.780\n0.091\n2.637\n2.969\n0.010\n0.007\n92.0\n24.0\n1.03\n\n\ne[1]\n2.624\n0.066\n2.499\n2.750\n0.002\n0.001\n1140.0\n1341.0\n1.01\n\n\nsigma\n3.861\n0.110\n3.664\n4.085\n0.002\n0.002\n2249.0\n1910.0\n1.00\n\n\n\n\n\n\n\n\n\nCode\naz.plot_ppc(data=idata_height_exp_male, group=\"posterior\", kind=\"kde\", num_pp_samples=500);\n\n\n\n\n\n\n\nCode\naz.plot_lm(idata=idata_height_exp_male, y='mass', x='height')\n\n\narray([[&lt;Axes: xlabel='height', ylabel='mass'&gt;]], dtype=object)\n\n\n\n\n\n\n\nCode\n!quarto render height.ipynb --to html\n\n\npandoc \n  to: html\n  output-file: height.html\n  standalone: true\n  section-divs: true\n  html-math-method: mathjax\n  wrap: none\n  default-image-extension: png\n  \nmetadata\n  document-css: false\n  link-citations: true\n  date-format: long\n  lang: en\n  title: Analysis of Height\n  \nOutput created: height.html"
  },
  {
    "objectID": "posts/flights/flight_delays.html",
    "href": "posts/flights/flight_delays.html",
    "title": "Flight Delays",
    "section": "",
    "text": "Code\nimport arviz as az\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\n\n\n\n\nCode\ndf = pd.read_csv(\"948363589_T_ONTIME_MARKETING.zip\")\n\n\n/var/folders/_f/1b2jxqtd7dvfvp83s99ygc4w0000gn/T/ipykernel_30196/1425327330.py:1: DtypeWarning: Columns (11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(\"948363589_T_ONTIME_MARKETING.zip\")\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\nYEAR\nQUARTER\nMONTH\nDAY_OF_MONTH\nDAY_OF_WEEK\nFL_DATE\nMKT_UNIQUE_CARRIER\nBRANDED_CODE_SHARE\nMKT_CARRIER_AIRLINE_ID\nMKT_CARRIER\n...\nARR_DELAY\nARR_DELAY_NEW\nARR_DEL15\nARR_DELAY_GROUP\nARR_TIME_BLK\nCANCELLED\nCANCELLATION_CODE\nDIVERTED\nDUP\nUnnamed: 60\n\n\n\n\n0\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n6.0\n6.0\n0.0\n0.0\n1100-1159\n0.0\nNaN\n0.0\nN\nNaN\n\n\n1\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-21.0\n0.0\n0.0\n-2.0\n2100-2159\n0.0\nNaN\n0.0\nN\nNaN\n\n\n2\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n10.0\n10.0\n0.0\n0.0\n1900-1959\n0.0\nNaN\n0.0\nN\nNaN\n\n\n3\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-10.0\n0.0\n0.0\n-1.0\n0900-0959\n0.0\nNaN\n0.0\nN\nNaN\n\n\n4\n2018\n4\n10\n18\n4\n2018-10-18\nUA\nUA\n19977\nUA\n...\n-10.0\n0.0\n0.0\n-1.0\n1300-1359\n0.0\nNaN\n0.0\nN\nNaN\n\n\n\n\n5 rows Ã— 61 columns\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(10,4))\n\nmsn_arrivals = df[(df[\"DEST\"] == \"MSN\") & df[\"ORIGIN\"].isin([\"MSP\", \"DTW\"])][\"ARR_DELAY\"]\n\naz.plot_kde(msn_arrivals.values, ax=ax, bw=10)\nax.set_yticks([])\nax.set_xlabel(\"Minutes late\")\n\n\nText(0.5, 0, 'Minutes late')\n\n\n\n\n\n\n\nCode\nwith pm.Model() as normal_model:\n    normal_sd = pm.HalfStudentT(\"sd\",sigma=60, nu=5)\n    normal_mu = pm.Normal(\"mu\", 0, 30) \n\n    normal_delay = pm.Normal(\"delays\",mu=normal_mu,\n                             sigma=normal_sd, observed=msn_arrivals)\n    normal_prior_predictive = pm.sample_prior_predictive()\n    \nwith pm.Model() as gumbel_model:\n    gumbel_beta = pm.HalfStudentT(\"beta\", sigma=60, nu=5)\n    gumbel_mu = pm.Normal(\"mu\", 0, 40)\n    \n    gumbel_delays = pm.Gumbel(\"delays\",\n                              mu=gumbel_mu,\n                              beta=gumbel_beta,\n                              observed=msn_arrivals)\n    gumbel_prior_predictive = pm.sample_prior_predictive()\n\n\nSampling: [delays, mu, sd]\nSampling: [beta, delays, mu]\n\n\n\n\nCode\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nprior_predictives = {\"normal\":normal_prior_predictive.prior_predictive, \"gumbel\": gumbel_prior_predictive.prior_predictive}\n\nfor i, (label, prior_predictive) in enumerate(prior_predictives.items()):\n    \n    data = prior_predictive[\"delays\"]\n    az.plot_dist(data, ax=axes[i])\n    axes[i].set_yticks([])\n    axes[i].set_xlim(-300, 300)\n    axes[i].set_title(label)\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    normal_delay_trace = pm.sample(random_seed=0, chains=2)\naz.plot_rank(normal_delay_trace)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [sd, mu]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 1 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\narray([&lt;Axes: title={'center': 'mu'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;,\n       &lt;Axes: title={'center': 'sd'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\nwith gumbel_model:\n    gumbel_delay_trace = pm.sample(chains=2)\naz.plot_rank(gumbel_delay_trace)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (2 chains in 4 jobs)\nNUTS: [beta, mu]\nSampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 0 seconds.\nWe recommend running at least 4 chains for robust computation of convergence diagnostics\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00 Sampling 2 chains, 0 divergences]\n    \n    \n\n\narray([&lt;Axes: title={'center': 'mu'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;,\n       &lt;Axes: title={'center': 'beta'}, xlabel='Rank (all chains)', ylabel='Chain'&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\naz.plot_posterior(normal_delay_trace)\n\n\narray([&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'sd'}&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\naz.plot_posterior(gumbel_delay_trace)\n\n\narray([&lt;Axes: title={'center': 'mu'}&gt;, &lt;Axes: title={'center': 'beta'}&gt;],\n      dtype=object)\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    normal_delay_trace = pm.sample(random_seed=0)\n    pm.sample_posterior_predictive(normal_delay_trace, extend_inferencedata=True)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [sd, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 1 seconds.\nSampling: [delays]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(normal_delay_trace, num_pp_samples=100)\n\n\n&lt;Axes: xlabel='delays / delays'&gt;\n\n\n\n\n\n\n\nCode\nwith gumbel_model:\n    gumbel_delay_trace = pm.sample(random_seed=0)\n    pm.sample_posterior_predictive(gumbel_delay_trace, extend_inferencedata=True)\n\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [beta, mu]\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 0 seconds.\nSampling: [delays]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:00&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\naz.plot_ppc(gumbel_delay_trace, num_pp_samples=100)\n\n\n&lt;Axes: xlabel='delays / delays'&gt;\n\n\n\n\n\n\n\nCode\ngumbel_late = gumbel_delay_trace.posterior_predictive[\"delays\"].values.reshape(-1, 336).copy()\ndist_of_late = (gumbel_late &gt; 0).sum(axis=1) / 336\n\npercent_observed_late = (msn_arrivals &gt; 0).sum() / 336\n\n\n\n\nCode\nfig, axes = plt.subplots(1,2, figsize=(12,4))\naz.plot_dist(dist_of_late, ax=axes[0])\n\n\naxes[0].axvline(percent_observed_late, c=\"gray\")\naxes[0].set_title(\"Test Statistic of On Time Proportion\")\naxes[0].set_yticks([])\n\ngumbel_late[gumbel_late &lt; 0] = np.nan\nmedian_lateness = np.nanmedian(gumbel_late, axis=1)\naz.plot_dist(median_lateness,  ax=axes[1])\n\nmedian_time_observed_late = msn_arrivals[msn_arrivals &gt;= 0].median()\naxes[1].axvline(median_time_observed_late, c=\"gray\")\naxes[1].set_title(\"Test Statistic of Median Minutes Late\")\naxes[1].set_yticks([])\n\n\n[]\n\n\n\n\n\n\n\nCode\nwith normal_model:\n    pm.compute_log_likelihood(normal_delay_trace)\n    \nwith gumbel_model:\n    pm.compute_log_likelihood(gumbel_delay_trace)\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:00&lt;00:00]\n    \n    \n\n\n\n\nCode\ncompare = az.compare({\"normal\": normal_delay_trace, \"gumbel\": gumbel_delay_trace}, ic=\"loo\")\ncompare\n\n\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:803: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.\n  warnings.warn(\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'False' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/stats/stats.py:307: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'log' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n  df_comp.loc[val] = (\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\ngumbel\n0\n-1410.330975\n5.866211\n0.000000\n1.000000e+00\n45.179994\n0.000000\nFalse\nlog\n\n\nnormal\n1\n-1653.563218\n21.263250\n243.232243\n1.609379e-09\n65.204290\n27.489639\nTrue\nlog\n\n\n\n\n\n\n\n\n\nCode\n_, axes = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\nfor label, model, ax in zip((\"gumbel\", \"normal\"),(gumbel_delay_trace, normal_delay_trace), axes):\n    az.plot_loo_pit(model, y=\"delays\", legend=False, use_hdi=True, ax=ax)\n    ax.set_title(label)\n\n\n\n\n\n\n\nCode\naz.plot_compare(compare);\n\n\n/Users/madschr.hansen/Library/Caches/pypoetry/virtualenvs/pymc-experiment-7JkXVX9c-py3.11/lib/python3.11/site-packages/arviz/plots/backends/matplotlib/compareplot.py:87: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n  scale = comp_df[\"scale\"][0]\n\n\n\n\n\n\n\nCode\n@np.vectorize\ndef current_revenue(delay):\n    if delay &gt;= 0:\n        return 300 * delay\n    return np.nan\n\n\n\n\nCode\nposterior_pred = gumbel_delay_trace.posterior_predictive[\"delays\"].values.reshape(-1, 336).copy()\n\n\n\n\nCode\ndef revenue_calculator(posterior_pred, revenue_func):    \n    revenue_per_flight = revenue_func(posterior_pred)\n    average_revenue = np.nanmean(revenue_per_flight)\n    return revenue_per_flight, average_revenue\n\nrevenue_per_flight, average_revenue = revenue_calculator(posterior_pred,\ncurrent_revenue)\naverage_revenue\n\n\n3929.7925096256663\n\n\n\n\nCode\nfig, ax = plt.subplots()\nax.hist(revenue_per_flight.flatten(), bins=30, rwidth=.9, color=\"C2\" )\nax.set_yticks([])\nax.set_title(\"Late fee revenue per flight under current fee structure\")\nax.xaxis.set_major_formatter('${x:1.0f}')\n\n\n\n\n\n\n\nCode\n@np.vectorize\ndef proposed_revenue(delay):\n    \"\"\"Calculate proposed revenue for each delay \"\"\"\n    if delay &gt;= 100:\n        return 30000\n    elif delay &gt;= 10:\n        return 5000\n    elif delay &gt;= 0:\n        return 1000\n    else:\n        return np.nan\nrevenue_per_flight_proposed, average_revenue_proposed = revenue_calculator(posterior_pred, proposed_revenue)\n\n\n\n\nCode\naverage_revenue_proposed\n\n\n2929.739994157172\n\n\n\n\nCode\nfig, ax = plt.subplots()\n\ncounts = pd.Series(revenue_per_flight_proposed.flatten()).value_counts()\ncounts.index = counts.index.astype(int)\n\ncounts.plot(kind=\"bar\", ax=ax, color=\"C2\")\nax.set_title(\"Late fee revenue per flight under proposed fee structure\")\nax.set_yticks([]);\nax.tick_params(axis='x', labelrotation = 0)\nax.set_xticklabels([f\"${i}\" for i in counts.index])\n\n\n[Text(0, 0, '$1000'), Text(1, 0, '$5000'), Text(2, 0, '$30000')]\n\n\n\n\n\n\n\nCode\n!quarto render flight_delays.ipynb --to html\n\n\npandoc \n  to: html\n  output-file: flight_delays.html\n  standalone: true\n  section-divs: true\n  html-math-method: mathjax\n  wrap: none\n  default-image-extension: png\n  \nmetadata\n  document-css: false\n  link-citations: true\n  date-format: long\n  lang: en\n  \nOutput created: flight_delays.html"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a mathematician (PhD from University of Copenhagen) and data scientist (Maersk). On this site you can find more information about me and some of my projects."
  },
  {
    "objectID": "about.html#research-and-interests",
    "href": "about.html#research-and-interests",
    "title": "About",
    "section": "Research and Interests",
    "text": "Research and Interests\nMy research interests lie within dynamical systems theory, both deterministic and stochastic, especially within reaction networks.\nI also enjoy working on statistical properties of dynamical systems within time-series forecasting, and Iâ€™m interested in all aspects of Bayesian data analysis.\n\nPapers:\n\nExistence of a unique quasi-stationary distribution in stochastic reaction networks, Electron. J. Probab. 25: 1-30 (2020)\nStructural classification of continuous time Markov chains with applications, Stochastics, 94:7, 1003-1030 (2022)\nFull classification of dynamics for one-dimensional continuous-time Markov chains with polynomial transition rates, Advances in Applied Probability , Volume 55, Issue 1, pp.Â 321 - 355 (2023)\nThe asymptotic tails of limit distributions of continuous time Markov chains, Advances in Applied Probability Volume 56, Issue 2 (to appear 2024)\n\n\n\nTech Stack:"
  }
]