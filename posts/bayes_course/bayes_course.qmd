---
title: "Bayes Course"
description: "Part 1"
author: "Mads Chr. Hansen"
date: "2023-10-28"
categories: [Theory]
image: "https://images.unsplash.com/photo-1456513080510-7bf3a84b82f8?auto=format&fit=crop&q=80&w=1373&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D"
format:
  html:
    code-fold: true
    code-tools: true
toc: true
#citation: true
callout-appearance: simple
bibliography: ../../references.bib
---

# Bayesian Inference

This presentation follows @Gelman2014 closely. Bayesian inference is about learning unobserved parameters $\theta$ or data $\tilde{y}$ though probability. These probability statements are conditional on the observed data $y$, that is $p(\theta|y)$ or $p(\tilde{y}|y)$. We also inplicitly condition on known covariates $x$.

We may write for example $\theta \sim N(\mu,\sigma^2)$ or $p(\theta)=N(\theta|\mu,\sigma^2)$, which is short hand for $p(\theta|\mu,\sigma^2) = N(\theta| \mu,\sigma^2)$.

## Bayes' Rule

To make probability statements about parameters $\theta$ given the data $y$, we must specify a *model* in terms of the joint distribution for $\theta$ and $y$. Thic can be written as the product of the prior $p(\theta)$ and the sampling distribution $p(y|\theta)$,
$$p(\theta, y) = p(y|\theta)p(\theta).$$
Conditioning on the known data yields the posterior density
$$p(\theta|y) = \frac{p(\theta, y)}{p(y)}=\frac{p(y|\theta)p(\theta)}{p(y)}$$
Note that by integrating over all possible values of $\theta$, we may actually write
$$p(y)=\int_{\theta}p(y, \theta)\,d\theta=\int_\theta p(y|\theta)p(\theta)\,d\theta$$
wehereby we arrive at the following fundamental identity of Bayesion inference
$$p(\theta|y) =\frac{p(y|\theta)p(\theta)}{\int_\theta p(y|\theta)p(\theta)\,d\theta}$${#eq-posterior}

## Prediction

To make inferences about an unknown observable i.e. predictive inference, we may reason similarly. After the data $y$ has been observed, we can predicct $\tilde{y}$ as follows
$$
\begin{align}
p(\tilde{y}|y)&= \int_\theta p(\tilde{y}, \theta| y)\,d\theta\\
&=\int_\theta p(\tilde{y}| \theta, y)p(\theta|y)\,d\theta\\
&= \int_\theta p(\tilde{y}| \theta)p(\theta|y)\,d\theta
\end{align}
$$
Thus it is the weighted sum of all possible values with the weight being the updated posterior probability informed by the $y$ observations.

## Likelihood

h
